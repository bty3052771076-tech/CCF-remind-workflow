#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æŠ“å–å™¨ - ä½¿ç”¨Pythonæ ‡å‡†åº“æŠ“å–ä¼šè®®ä¿¡æ¯
æ”¯æŒä»å¤šä¸ªæ•°æ®æºè·å–ä¼šè®®æ•°æ®å¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
"""

import urllib.request
import urllib.error
import json
import re
import sys
from datetime import datetime
from html.parser import HTMLParser
from typing import List, Dict, Optional
import time

# Windowsæ§åˆ¶å°ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    import codecs
    if hasattr(sys.stdout, 'buffer'):
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)


class DataFetcher:
    """æ•°æ®æŠ“å–å™¨ - ä½¿ç”¨æ ‡å‡†åº“urllibæŠ“å–ç½‘é¡µæ•°æ®"""

    def __init__(self, sources_file: str = 'sources.json'):
        """åˆå§‹åŒ–æ•°æ®æŠ“å–å™¨

        Args:
            sources_file: æ•°æ®æºé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.sources = self._load_sources(sources_file)
        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        self.timeout = 30

    def _load_sources(self, sources_file: str) -> List[Dict]:
        """åŠ è½½æ•°æ®æºé…ç½®"""
        try:
            with open(sources_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('sources', [])
        except FileNotFoundError:
            print(f"âš ï¸  æ•°æ®æºé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {sources_file}")
            print("   è¯·å…ˆåˆ›å»º sources.json é…ç½®æ–‡ä»¶")
            return []
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            return []

    def fetch_page(self, url: str, retries: int = 3) -> Optional[str]:
        """æŠ“å–ç½‘é¡µå†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            url: ç›®æ ‡URL
            retries: é‡è¯•æ¬¡æ•°

        Returns:
            ç½‘é¡µHTMLå†…å®¹ï¼ˆå¤±è´¥è¿”å›Noneï¼‰
        """
        for attempt in range(retries):
            try:
                req = urllib.request.Request(
                    url,
                    headers={
                        'User-Agent': self.user_agent,
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
                    }
                )

                with urllib.request.urlopen(req, timeout=self.timeout) as response:
                    # å°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                    content = response.read()

                    # ä¼˜å…ˆä»HTTPå¤´è·å–ç¼–ç 
                    charset = None
                    content_type = response.getheader('Content-Type', '')
                    if 'charset=' in content_type:
                        charset = content_type.split('charset=')[-1].strip()

                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¼–ç ï¼Œå°è¯•ä»metaæ ‡ç­¾è·å–
                    if not charset:
                        try:
                            html = content.decode('utf-8', errors='ignore')
                            match = re.search(r'<meta[^>]+charset=["\']?([^"\'>\s]+)', html, re.I)
                            if match:
                                charset = match.group(1)
                        except:
                            pass

                    # é»˜è®¤ä½¿ç”¨utf-8
                    if not charset:
                        charset = 'utf-8'

                    try:
                        return content.decode(charset)
                    except (UnicodeDecodeError, LookupError):
                        # å¦‚æœæŒ‡å®šç¼–ç å¤±è´¥ï¼Œå°è¯•å¸¸è§ç¼–ç 
                        for fallback_encoding in ['utf-8', 'gbk', 'gb2312', 'iso-8859-1']:
                            try:
                                return content.decode(fallback_encoding)
                            except:
                                continue

                        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨utf-8å¹¶å¿½ç•¥é”™è¯¯
                        return content.decode('utf-8', errors='ignore')

            except urllib.error.HTTPError as e:
                print(f"âŒ HTTPé”™è¯¯ (å°è¯• {attempt + 1}/{retries}): {e.code} - {url}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                return None

            except urllib.error.URLError as e:
                print(f"âŒ ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{retries}): {e.reason} - {url}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return None

            except Exception as e:
                print(f"âŒ æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{retries}): {e} - {url}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return None

        return None

    def fetch_from_source(self, source_id: str) -> List[Dict]:
        """ä»æŒ‡å®šæ•°æ®æºè·å–æ•°æ®

        Args:
            source_id: æ•°æ®æºID

        Returns:
            æ ‡å‡†åŒ–åçš„ä¼šè®®æ•°æ®åˆ—è¡¨
        """
        source = next((s for s in self.sources if s['id'] == source_id), None)
        if not source:
            print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æº: {source_id}")
            return []

        if not source.get('enabled', True):
            print(f"â­ï¸  æ•°æ®æºå·²ç¦ç”¨: {source_id}")
            return []

        print(f"ğŸ“¡ æ­£åœ¨ä» {source['name']} æŠ“å–æ•°æ®...")

        # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©è§£æå™¨
        parser_type = source.get('parser', source_id)

        try:
            if parser_type == 'ccfddl':
                data = self._parse_ccfddl(source['url'])
            elif parser_type == 'ccf_official':
                data = self._parse_ccf_official(source['url'])
            elif parser_type == 'manual':
                # æ‰‹åŠ¨æ•°æ®æºï¼Œç›´æ¥è¿”å›é¢„å®šä¹‰æ•°æ®
                data = source.get('data', [])
            else:
                print(f"âš ï¸  ä¸æ”¯æŒçš„è§£æå™¨ç±»å‹: {parser_type}")
                return []

            # æ ‡å‡†åŒ–æ•°æ®
            normalized_data = [
                self.normalize_conference(conf, source_id)
                for conf in data
            ]

            print(f"âœ… æˆåŠŸæŠ“å– {len(normalized_data)} æ¡æ•°æ®")
            return normalized_data

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def fetch_all_enabled_sources(self) -> Dict[str, List[Dict]]:
        """ä»æ‰€æœ‰å¯ç”¨çš„æ•°æ®æºè·å–æ•°æ®

        Returns:
            {source_id: conferences_list} çš„å­—å…¸
        """
        all_data = {}

        enabled_sources = [s for s in self.sources if s.get('enabled', True)]

        if not enabled_sources:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„æ•°æ®æº")
            return all_data

        print(f"ğŸ“Š å…±æœ‰ {len(enabled_sources)} ä¸ªå¯ç”¨çš„æ•°æ®æº")

        for source in enabled_sources:
            print(f"\n{'='*60}")
            data = self.fetch_from_source(source['id'])
            all_data[source['id']] = data

        return all_data

    def _parse_ccfddl(self, url: str) -> List[Dict]:
        """è§£æccfddl.topç½‘ç«™

        è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è§£æå™¨ï¼Œå®é™…å®ç°éœ€è¦æ ¹æ®ç½‘ç«™ç»“æ„è°ƒæ•´
        """
        print(f"ğŸ” æ­£åœ¨è§£æ {url}...")

        html = self.fetch_page(url)
        if not html:
            return []

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä¼šè®®ä¿¡æ¯
        # æ³¨æ„ï¼šè¿™é‡Œæ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹ï¼Œå®é™…éœ€è¦æ ¹æ®ç½‘ç«™HTMLç»“æ„è°ƒæ•´
        conferences = []

        # ç¤ºä¾‹ï¼šæå–åŒ…å«ä¼šè®®ä¿¡æ¯çš„div
        # å®é™…å®ç°æ—¶éœ€è¦ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·æŸ¥çœ‹ç½‘ç«™ç»“æ„
        pattern = re.compile(
            r'<div[^>]*class="[^"]*conf[^"]*"[^>]*>.*?'
            r'<h3[^>]*>(.*?)</h3>.*?'
            r'(?:deadline|æˆªæ­¢)[^>]*>([^<]+)</span>.*?'
            r'(?:rank|ç­‰çº§)[^>]*>([ABC])</span>',
            re.DOTALL | re.IGNORECASE
        )

        matches = pattern.findall(html)

        for name, deadline, rank in matches:
            # æ¸…ç†HTMLæ ‡ç­¾
            name = re.sub(r'<[^>]+>', '', name).strip()
            deadline = re.sub(r'[^\d-]', '', deadline).strip()

            if name and deadline:
                conferences.append({
                    'name': name,
                    'deadline': deadline,
                    'rank': rank,
                    'conference_date': '',
                    'website': '',
                    'description': ''
                })

        return conferences

    def _parse_ccf_official(self, url: str) -> List[Dict]:
        """è§£æCCFå®˜æ–¹ç½‘ç«™

        è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è§£æå™¨ï¼Œå®é™…å®ç°éœ€è¦æ ¹æ®ç½‘ç«™ç»“æ„è°ƒæ•´
        """
        print(f"ğŸ” æ­£åœ¨è§£æ {url}...")

        html = self.fetch_page(url)
        if not html:
            return []

        # CCFå®˜ç½‘ä¸»è¦ç”¨äºéªŒè¯ä¼šè®®ç­‰çº§
        # è¿™é‡Œè¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å®˜ç½‘ç»“æ„å®ç°è§£æ
        print("âš ï¸  CCFå®˜ç½‘è§£æå™¨å¾…å®ç°")
        return []

    def normalize_conference(self, raw_conf: Dict, source_id: str) -> Dict:
        """æ ‡å‡†åŒ–ä¼šè®®æ•°æ®æ ¼å¼

        Args:
            raw_conf: åŸå§‹ä¼šè®®æ•°æ®
            source_id: æ•°æ®æºID

        Returns:
            æ ‡å‡†åŒ–åçš„ä¼šè®®æ•°æ®
        """
        # æå–å¹¶æ¸…ç†æˆªæ­¢æ—¥æœŸ
        deadline = raw_conf.get('deadline', '')
        if deadline:
            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{4})-(\d{1,2})-(\d{1,2})',  # YYYY-MM-DD
                r'(\d{4})/(\d{1,2})/(\d{1,2})',  # YYYY/MM/DD
                r'(\d{1,2})-(\d{1,2})-(\d{4})',  # DD-MM-YYYY
            ]

            for pattern in date_patterns:
                match = re.search(pattern, deadline)
                if match:
                    groups = match.groups()
                    # æ ‡å‡†åŒ–ä¸º YYYY-MM-DD æ ¼å¼
                    if len(groups[0]) == 4:  # å¹´åœ¨å‰
                        deadline = f"{groups[0]}-{groups[1].zfill(2)}-{groups[2].zfill(2)}"
                    else:  # å¹´åœ¨å
                        deadline = f"{groups[2]}-{groups[1].zfill(2)}-{groups[0].zfill(2)}"
                    break

        # æ¸…ç†ä¼šè®®åç§°
        name = raw_conf.get('name', '').strip()
        name = re.sub(r'\s+', ' ', name)  # åˆå¹¶å¤šä¸ªç©ºæ ¼

        # æ ‡å‡†åŒ–ç­‰çº§
        rank = raw_conf.get('rank', '').upper().strip()
        if rank not in ['A', 'B', 'C']:
            rank = 'N/A'

        # ç”Ÿæˆå”¯ä¸€ID
        conf_id = self._generate_conf_id(name, raw_conf.get('deadline', ''))

        normalized = {
            'id': conf_id,
            'name': name,
            'rank': rank,
            'deadline': deadline,
            'conference_date': raw_conf.get('conference_date', ''),
            'website': raw_conf.get('website', ''),
            'description': raw_conf.get('description', ''),
            'type': raw_conf.get('type', 'conference'),
            'fields': raw_conf.get('fields', []),
            'source_id': source_id,
            'raw_data': raw_conf  # ä¿ç•™åŸå§‹æ•°æ®ä»¥ä¾¿è°ƒè¯•
        }

        return normalized

    def _generate_conf_id(self, name: str, deadline: str) -> str:
        """ç”Ÿæˆä¼šè®®å”¯ä¸€ID

        Args:
            name: ä¼šè®®åç§°
            deadline: æˆªæ­¢æ—¥æœŸ

        Returns:
            å”¯ä¸€IDå­—ç¬¦ä¸²
        """
        # æå–ä¼šè®®ç¼©å†™ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªè¯æˆ–è¿ç»­å¤§å†™å­—æ¯ï¼‰
        abbrev_match = re.search(r'\b([A-Z]{2,})\b', name)
        if abbrev_match:
            abbrev = abbrev_match.group(1).lower()
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¼©å†™ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå•è¯
            first_word = name.split()[0].lower()
            abbrev = re.sub(r'[^a-z0-9]', '', first_word)[:10]

        # æå–å¹´ä»½
        year_match = re.search(r'\b(20\d{2})\b', name + ' ' + deadline)
        year = year_match.group(1) if year_match else '0000'

        return f"{abbrev}-{year}"

    def save_to_file(self, data: List[Dict], filename: str):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶

        Args:
            data: ä¼šè®®æ•°æ®åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        # ç§»é™¤raw_dataå­—æ®µï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰
        clean_data = [
            {k: v for k, v in conf.items() if k != 'raw_data'}
            for conf in data
        ]

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(clean_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")


class ConferenceHTMLParser(HTMLParser):
    """HTMLè§£æå™¨åŸºç±»ï¼Œç”¨äºè§£æä¼šè®®ç½‘ç«™"""

    def __init__(self):
        super().__init__()
        self.conferences = []
        self.current_conf = {}
        self.in_conference = False
        self.current_tag = None
        self.current_data = []

    def handle_starttag(self, tag, attrs):
        """å¤„ç†å¼€å§‹æ ‡ç­¾"""
        attrs_dict = dict(attrs)

        # æ£€æµ‹æ˜¯å¦è¿›å…¥ä¼šè®®é¡¹
        if tag in ['div', 'li', 'article']:
            classes = attrs_dict.get('class', '').split()
            if any(cls in ['conf', 'conference', 'item'] for cls in classes):
                self.in_conference = True
                self.current_conf = {}

        # è®°å½•å½“å‰æ ‡ç­¾
        if self.in_conference:
            self.current_tag = tag

    def handle_endtag(self, tag):
        """å¤„ç†ç»“æŸæ ‡ç­¾"""
        # æ£€æµ‹æ˜¯å¦ç¦»å¼€ä¼šè®®é¡¹
        if tag in ['div', 'li', 'article'] and self.in_conference:
            self._save_current_conference()

        self.current_tag = None

    def handle_data(self, data):
        """å¤„ç†æ–‡æœ¬æ•°æ®"""
        if self.in_conference and self.current_tag:
            self.current_data.append(data.strip())

    def _save_current_conference(self):
        """ä¿å­˜å½“å‰ä¼šè®®"""
        if self.current_conf:
            self.conferences.append(self.current_conf)
            self.current_conf = {}
            self.in_conference = False


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='ä¼šè®®æ•°æ®æŠ“å–å·¥å…·')
    parser.add_argument('--sources', type=str, default='sources.json',
                       help='æ•°æ®æºé…ç½®æ–‡ä»¶ (é»˜è®¤: sources.json)')
    parser.add_argument('--source', type=str,
                       help='åªæŠ“å–æŒ‡å®šçš„æ•°æ®æºID')
    parser.add_argument('--output', type=str,
                       help='è¾“å‡ºæ–‡ä»¶åï¼ˆJSONæ ¼å¼ï¼‰')
    parser.add_argument('--verbose', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    args = parser.parse_args()

    print("="*60)
    print("ğŸ“Š ä¼šè®®æ•°æ®æŠ“å–å·¥å…·")
    print("="*60)

    # åˆ›å»ºæŠ“å–å™¨
    fetcher = DataFetcher(args.sources)

    # æŠ“å–æ•°æ®
    if args.source:
        # æŠ“å–å•ä¸ªæ•°æ®æº
        data = fetcher.fetch_from_source(args.source)
        all_data = {args.source: data}
    else:
        # æŠ“å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®æº
        all_data = fetcher.fetch_all_enabled_sources()

    # ç»Ÿè®¡
    total_conferences = sum(len(confs) for confs in all_data.values())
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æŠ“å–å®Œæˆï¼")
    print(f"   æ•°æ®æºæ•°é‡: {len(all_data)}")
    print(f"   ä¼šè®®æ€»æ•°: {total_conferences}")

    for source_id, confs in all_data.items():
        print(f"   - {source_id}: {len(confs)} æ¡")

    # ä¿å­˜åˆ°æ–‡ä»¶
    if args.output:
        # åˆå¹¶æ‰€æœ‰æ•°æ®æº
        merged_data = []
        for source_id, confs in all_data.items():
            merged_data.extend(confs)

        fetcher.save_to_file(merged_data, args.output)

    print("="*60)


if __name__ == '__main__':
    main()
